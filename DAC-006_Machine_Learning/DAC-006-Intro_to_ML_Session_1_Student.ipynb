{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** \\\n",
    "Machine Learning focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model is trained on a dataset to identify patterns and make predictions or decisions without human intervention.\n",
    "\n",
    "**A general pipeline:** \\\n",
    "1.) Data Collection - Collecting data relevant to the problem you want to solve. \\\n",
    "2.) Training  - Using this dataset to train a model, which involves adjusting the model's parameters to minimize errors in its predictions. \\\n",
    "3.) Evaluation - Testing the model on new, unseen data to evaluate its performance. \\\n",
    "4.) Prediction + Deployment - Using the trained model to make predictions or decisions on new data, and deploying it so that other users may use it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate (Simple) Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate linear regression is a statistical method used to model the relationship between a single independent variable $x$ and a dependent variable $y$ by fitting a linear equation to observed data. It aims to predict the dependent variable based on the value of the independent variable.\n",
    "\n",
    "equation of line: $$y_i = wx_i + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = wx_i + b$$\n",
    "\n",
    "How do you generate a line? You need a value for the Slope and Intercept. Use Least Squares method / Maximum Likelihood Estimation to determine. \n",
    "\n",
    "Objective: \\\n",
    "Minimise the sum of squared error terms ie: $$ \\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i (y_i - \\hat{y}_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical to the univariate case except you're now modelling the relationship between $y$ with multiple other independent variables/features $x_1,x_2...x_n$.\n",
    "\n",
    "equation of line: $$y_i = w_1x_{1,i} + w_2x_{2,i} + ...  + b + \\epsilon_i$$\n",
    "equation of your prediction: $$ \\hat{y_i} = w_1x_{1,i} + w_2x_{2,i} + ... + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics can be used **Mean Squared Error (MSE)** and **$R^2$ value**.\n",
    "\n",
    "We split our model into training data and validation/test data. Apply regression line fitted on train data into validation data to evaluate performance. MSE is pretty self explanatory, $R^2$ measures the proportion of variance of the dependent/target feature that is explained by the independent features. \n",
    "\n",
    "$$\n",
    " MSE = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    " R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "MSE is also known as the loss function - a function that maps events or values of variables onto a real number intuitively representing some \"cost\" associated with the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when your model fits the training data too well; performs worse on test data. \\\n",
    "Regularization tries to prevent this by adding a penalty term to a model's loss function.\n",
    "\n",
    "In Linear Regression:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 + \\text{Regularization Term}\n",
    "$$\n",
    "Your left term reduces losses, right term prevents losses from decreasing excessively. Now let's see this in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Import our libraries \n",
    "!pip install scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Datasets/Boston.csv\")\n",
    "data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = ax + b + Gaussian noise \n",
    "# def reg_data(a, b, n, s):\n",
    "#    rtn_x, rtn_y = [], []\n",
    "#    for i in range(n):\n",
    "#        x = np.random.normal(0.0, 0.5)\n",
    "#        y = a * x + b + np.random.normal(0.0, s)\n",
    "#        rtn_x.append(x) # input features\n",
    "#        rtn_y.append(y) # target values\n",
    "#    return np.array(rtn_x).reshape(-1,1), np.array(rtn_y)\n",
    "\n",
    "# # Generate 1,000 data points drawn from y = ax + b + noise\n",
    "# # s : standard deviation of the noise distribution\n",
    "# x, y = reg_data(a=0.5, b=0.3, n=1000, s=0.2)\n",
    "\n",
    "# # y = w0 + w1*x1 + w2*x2 + ... â†’ w0*x0 + w1*x1 + w2*x2 + ... (x0 = 1)\n",
    "# # y = [w0, w1, w2, ...] * [x0, x1, x2, ...].T  (T : transpose)\n",
    "# # y = W * X.T\n",
    "# X = np.hstack([np.ones([x.shape[0], 1]), x]) # horizontally stack a column of ones (intercept) with input features\n",
    "# REG_CONST = 0.01   # regularization constant\n",
    "\n",
    "# # Loss function : Mean Squared Error\n",
    "# def ols_loss(W, args):\n",
    "#     e = np.dot(W, X.T) - y\n",
    "#     mse = np.mean(np.square(e))  # mean squared error\n",
    "#     loss = mse + REG_CONST * np.sum(np.square(W)) # this is Ridge (L2) Regularization\n",
    "    \n",
    "#     # save W and loss\n",
    "#     if args[0] == True:\n",
    "#         trace_W.append([W, loss])\n",
    "#     return loss\n",
    "\n",
    "# # Perform optimization process\n",
    "# trace_W = []\n",
    "# result = optimize.minimize(ols_loss, [-4., 4], args=[True]) # minimise loss function starting from initial weights [-4,4]\n",
    "# print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for more info on the output visit: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "# # x: is the vector of the optimal solution\n",
    "\n",
    "# # Plot the training data and draw the regression line.\n",
    "# y_hat = np.dot(result.x, X.T) # predicted values using the optimized weights and the design matrix\n",
    "# plt.figure(figsize=(6, 6)) \n",
    "# plt.scatter(x, y, s=5, c='r')\n",
    "# plt.plot(x, y_hat, c='blue')\n",
    "# plt.axvline(x=0, ls='--', lw=0.5, c='black')\n",
    "# plt.axhline(y=0, ls='--', lw=0.5, c='black')\n",
    "# plt.show()\n",
    "\n",
    "# # Draw the loss function and the path to the optimal point.\n",
    "# m = 5\n",
    "# t = 0.1\n",
    "# w0, w1 = np.meshgrid(np.arange(-m, m, t), np.arange(-m, m, t))\n",
    "# zs = np.array([ols_loss([a,b], [False]) for [a, b] in zip(np.ravel(w0), np.ravel(w1))])\n",
    "# z = zs.reshape(w0.shape)\n",
    "\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Draw the surface of the loss function\n",
    "# ax.plot_surface(w0, w1, z, alpha=0.7)\n",
    "\n",
    "# # Draw the path to the optimal point.\n",
    "# b = np.array([tw0 for [tw0, tw1], td in trace_W])\n",
    "# w = np.array([tw1 for [tw0, tw1], td in trace_W])\n",
    "# d = np.array([td for [tw0, tw1], td in trace_W])\n",
    "# ax.plot(b, w, d, marker='o', color=\"r\")\n",
    "\n",
    "# ax.set_xlabel('W0 (bias)')\n",
    "# ax.set_ylabel('W1 (slope)')\n",
    "# ax.set_zlabel('distance')\n",
    "# ax.azim = -50\n",
    "# ax.elev = 50\n",
    "# plt.show()\n",
    "\n",
    "# # Check the R2 score\n",
    "# sst = np.sum(np.square(y - np.mean(y)))  # total sum of squares\n",
    "# sse = np.sum(np.square(y - y_hat))       # sum of squares of error\n",
    "# r2 = 1 - sse / sst\n",
    "# print('\\nR2 score = {:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Implementation in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling (Normalization/Standardization) is a technique that shifts data closer toward the origin and scales the different feature $x_i, x_j$ weights to ensure that they are not significantly different. If they are different, can also affect the estimation of our slope $w$ and intercept $b$. \\\n",
    "During regularization, it may also unfairly impose greater penalties on some coefficients over others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the Boston.csv dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\youxi\\onedrive\\desktop\\dac\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Import our libraries\n",
    "!pip install scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio  lstat  medv  \n",
       "0  296     15.3   4.98  24.0  \n",
       "1  242     17.8   9.14  21.6  \n",
       "2  242     17.8   4.03  34.7  \n",
       "3  222     18.7   2.94  33.4  \n",
       "4  222     18.7   5.33  36.2  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read our data into a dataframe \n",
    "data = pd.read_csv(\"Datasets/Boston.csv\")\n",
    "data.shape\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  506 non-null    int64  \n",
      " 1   crim        506 non-null    float64\n",
      " 2   zn          506 non-null    float64\n",
      " 3   indus       506 non-null    float64\n",
      " 4   chas        506 non-null    int64  \n",
      " 5   nox         506 non-null    float64\n",
      " 6   rm          506 non-null    float64\n",
      " 7   age         506 non-null    float64\n",
      " 8   dis         506 non-null    float64\n",
      " 9   rad         506 non-null    int64  \n",
      " 10  tax         506 non-null    int64  \n",
      " 11  ptratio     506 non-null    float64\n",
      " 12  lstat       506 non-null    float64\n",
      " 13  medv        506 non-null    float64\n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>253.500000</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>146.213884</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>127.250000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>253.500000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>379.750000</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        crim          zn       indus        chas         nox  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean   253.500000    3.613524   11.363636   11.136779    0.069170    0.554695   \n",
       "std    146.213884    8.601545   23.322453    6.860353    0.253994    0.115878   \n",
       "min      1.000000    0.006320    0.000000    0.460000    0.000000    0.385000   \n",
       "25%    127.250000    0.082045    0.000000    5.190000    0.000000    0.449000   \n",
       "50%    253.500000    0.256510    0.000000    9.690000    0.000000    0.538000   \n",
       "75%    379.750000    3.677083   12.500000   18.100000    0.000000    0.624000   \n",
       "max    506.000000   88.976200  100.000000   27.740000    1.000000    0.871000   \n",
       "\n",
       "               rm         age         dis         rad         tax     ptratio  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     6.284634   68.574901    3.795043    9.549407  408.237154   18.455534   \n",
       "std      0.702617   28.148861    2.105710    8.707259  168.537116    2.164946   \n",
       "min      3.561000    2.900000    1.129600    1.000000  187.000000   12.600000   \n",
       "25%      5.885500   45.025000    2.100175    4.000000  279.000000   17.400000   \n",
       "50%      6.208500   77.500000    3.207450    5.000000  330.000000   19.050000   \n",
       "75%      6.623500   94.075000    5.188425   24.000000  666.000000   20.200000   \n",
       "max      8.780000  100.000000   12.126500   24.000000  711.000000   22.000000   \n",
       "\n",
       "            lstat        medv  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)  # Removes missing values\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are what the column names represent:\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per $10,000\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "* MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio  lstat  medv  \n",
       "0  296     15.3   4.98  24.0  \n",
       "1  242     17.8   9.14  21.6  \n",
       "2  242     17.8   4.03  34.7  \n",
       "3  222     18.7   2.94  33.4  \n",
       "4  222     18.7   5.33  36.2  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the dataframe (try DataWrangler)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>lstat</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00632</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02731</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02729</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06905</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "crim                                                                        \n",
       "0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "         lstat  price  \n",
       "crim                   \n",
       "0.00632   4.98   24.0  \n",
       "0.02731   9.14   21.6  \n",
       "0.02729   4.03   34.7  \n",
       "0.03237   2.94   33.4  \n",
       "0.06905   5.33   36.2  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the first column into the index\n",
    "# alternatively, pd.read_csv('datasets/Boston.csv', index_col=0) can be used\n",
    "data.set_index(data.columns[0], inplace = True)\n",
    "# Rename the 'medv' column to 'price'\n",
    "data.rename(columns={'medv':'price'}, inplace = True)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Separate the features (x) and the target variable (y)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'"
     ]
    }
   ],
   "source": [
    "# Separate the features (x) and the target variable (y)\n",
    "y = data['price']\n",
    "x = data.drop(columns = ['price'])\n",
    "\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "``\n",
    "# Transform the test data using the same scaler\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [379, 127]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the R2 score\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m r2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nR2 (Linear Regression) = \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(r2))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 2. Ridge regularization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\sklearn\\base.py:849\u001b[0m, in \u001b[0;36mRegressorMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[0;32m    848\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m--> 849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1204\u001b[0m, in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[0;32m   1198\u001b[0m xp, _, device_ \u001b[38;5;241m=\u001b[39m get_namespace_and_device(\n\u001b[0;32m   1199\u001b[0m     y_true, y_pred, sample_weight, multioutput\n\u001b[0;32m   1200\u001b[0m )\n\u001b[0;32m   1202\u001b[0m dtype \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1204\u001b[0m _, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y_pred) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 111\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\youxi\\OneDrive\\Desktop\\DAC\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [379, 127]"
     ]
    }
   ],
   "source": [
    "# 1. LinearRegression() -> this applies mean centering internally to the data \n",
    "model = LinearRegression()\n",
    "model.fit(x_train_scaled, y_train)\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "# Visually check the predicted and actual y values â€‹â€‹of the test data.\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(y_test, y_pred, s=20, c='r')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = model.score(x_test_scaled,y_train)\n",
    "print('/nR2 (Linear Regression) = {:.3f}'.format(r2))\n",
    "\n",
    "# 2. Ridge regularization\n",
    "model = Ridge(alpha = 0.01)\n",
    "model.fit(x_train_scaled,y_test)\n",
    "r2 = model.score(x_test_scaled,y_train)\n",
    "print('/nR2 (Linear Regression) = {:.3f}'.format(r2))\n",
    "\n",
    "# 3. Lasso regularization\n",
    "model = Lasso(alpha = 0.01)\n",
    "model.fit(x_train_scaled,y_test)\n",
    "r2 = model.score(x_test_scaled,y_train)\n",
    "print('/nR2 (Linear Regression) = {:.3f}'.format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Weighted Linear Regression (LWLR) is a non-parametric algorithm that fits multiple linear regressions to different subsets of the data, giving more weight to points closer to the target point. This allows the model to capture local patterns and variations in the data, making it highly flexible and adaptive to changes in the data distribution.\n",
    "\n",
    "Weighted Cost Function - calculate distance $d$ between test data point $px$ and all training data points, and calculate weight $w$ for each datapoint with a normal distribution for $d$. \n",
    "\n",
    "$$\n",
    "d_i = |px - x_i| \\\\\n",
    "    \n",
    "w_i = \\exp\\left(-\\frac{d^2}{2\\tau^2}\\right) \\quad \n",
    "    \\begin{cases}\n",
    "        d_i \\to 0 : w_i \\to 1 \\\\\n",
    "        d_i \\to \\infty : w_i \\to 0\n",
    "    \\end{cases} \\\\\n",
    "\n",
    "\\\\\n",
    "    \n",
    "\n",
    "\\min_{w,b} \\sum_i \\epsilon_i^2 = \\min_{w,b} \\sum_i w_i(y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tau$ is the standard deviation of the normal distribution and can adjust the range of neighbours; $\\tau$ is a hyperparameter.\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the learning process begins and controls the behavior of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our data into a dataframe \n",
    "\n",
    "\n",
    "# Separate the features (x) and the target variable (y)\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "\n",
    "# Initialize the scaler\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: training data, test: test data point to be predicted\n",
    "\n",
    "# we set tau = 50.0\n",
    "\n",
    "# Visually check the actual and predicted y values â€‹â€‹of the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple (Binary) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method for analyzing datasets in which there are one or more independent $y$ variables that determine an outcome, used for binary classification problems. It estimates the probability that a given input point belongs to a certain class using a logistic function.\n",
    "\n",
    "logistic function formula:\n",
    "$$\n",
    "\\hat{y}_i = \\frac{1}{1 + e^{-(wx_i + b)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression, we used Maxmimum Likelihood Estimation (MLE) to generate an objective function. In the same way, Logistic Regression can also use MLE to generate an objective function that minimises binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in breast cancer dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength\n",
    "\n",
    "\n",
    "# Predict the classes of test data and measure the accuracy of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "\n",
    "\n",
    "## Plot ROC curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "## Create and plot confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# # Print AUC score\n",
    "# print(f'AUC Score: {roc_auc:.3f}')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Logistic Regression extends binary logistic regression to handle multiple classes by using techniques like one-vs-rest (OvR) or softmax regression. It estimates the probability of each class and assigns the input to the class with the highest probability.\n",
    "\n",
    "Here we will be looking at softmax regression. To obtain the loss function for softmax regression, we can use MLE and minimise cross entropy, which is a generalised form of binary cross entropy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in the iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have more things to process, let's break it down. We want to:\n",
    "\n",
    "1. Deal with categorical data \n",
    "2. Scale numeric values with a scaling function\n",
    "\n",
    "What other types of processes do we foresee having to do with our data? Hint: what about missing values? what about outliers?\n",
    "\n",
    "It would be tedious to go through all these processes manually - sklearn has a Pipeline class that simplifies these preprocessing/feature engineering steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and test data\n",
    "\n",
    "\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "\n",
    "# regularization constant (strength)\n",
    "REG_CONST = 0.01\n",
    "\n",
    "# Create a model and fit it to the training data.\n",
    "# C := inverse of regularization strength, stronger regularization with smaller values\n",
    "\n",
    "# Predict the classes of test data and measure the accuracy of test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get prediction probabilities\n",
    "# y_pred_proba = model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# # Create and plot confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "# disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "# print(f'F1 Score: {f1:.3f}')\n",
    "\n",
    "# # Print Classification Report \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
